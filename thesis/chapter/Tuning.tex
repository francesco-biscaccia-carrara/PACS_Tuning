Although the baseline PACS algorithm performs well on many MIP instances, its focus on integer variables and reliance on fixed parameters can limit diversification and lead to premature convergence.

This section presents enhancements aimed at overcoming these limitations:
\begin{enumerate}
\item Generalizing starting vector construction and variable fixing to include both integer and continuous variables.
\item Implementing architecture-agnostic parallelization for consistent behavior across hardware.
\item Removing manual parameter tuning via adaptive sub-MIP time limits and dynamic adjustment of $\rho$.
\item Introducing a parameter-free initial solution strategy.
\item Adding optimization procedures, including slack upper bound enforcement and WalkMIP-based variable fixing, to improve feasibility and solution quality.
\end{enumerate}

These improvements increase PACS’s robustness, efficiency, and applicability while preserving its strengths in diversification and parallelism.

\section{PACS with Generalized Fixing}
The baseline PACS algorithm enforces both the starting vector construction and the fixing scheme to operate exclusively on the set $\mathcal{I}$ of integer variables. While this restriction may appear efficient and straightforward, it can, in fact, be limiting. In particular, if the MIP instance contains only a small fraction of integer variables relative to the total, focusing solely on them may reduce diversification and cause the algorithm to converge prematurely to a local minimum, which is undesirable in an optimization process.
To overcome this issue, Algorithm \ref{alg:starting_vector} and Algorithm \ref{alg:variable_fixing} are generalized into Algorithm \ref{alg:gen_starting_vector} and Algorithm \ref{alg:gen_variable_fixing}, respectively, thereby allowing both integer and continuous variables to be considered.
\begin{algorithm}[H]
\caption{Generalized Starting vector heuristic}\label{alg:gen_starting_vector}
\begin{algorithmic}[1]
\Require{Percentage of variables to fix $\theta$, $0 < \theta \leq 100$, Fixed bound constant $c_b$}
\Ensure{Starting integer-feasible vector $\hat{x}$}
\State $V :=$ list of \cancel{integer} variables sorted by increasing bound range $u-l$
\State $F := \emptyset$
\While{$\hat{x}$ is not integer feasible \textbf{AND} $F \neq V$}
    \State $\mathcal{K} :=$ top $\theta \%$ of unfixed variables from $V$
    \For{$k \in \mathcal{K}$}
        \State $\hat{x}_k :=$ random integer value between $[\max(l_k, -c_b), \min(u_k, c_b)]$
    \EndFor
    \State $F := F \cup \mathcal{K}$
    \State $[x, \Delta^+, \Delta^-] := \min\{\sum_i \Delta_i^+ + \Delta_i^- \mid A x + I_m \Delta^+ - I_m \Delta^- = b, \; x_j = \hat{x}_j \;\; \forall j \in F\}$
    \State $Q :=$ index set of \cancel{integer} variables of $x$ with integer value
    \State $\hat{x}_q = x_q, \;\; \forall q \in Q$
    \State $F := F \cup Q$
\EndWhile
\State \Return $\hat{x}$
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
\caption{Generalized Variable Fixing Selection Algorithm}\label{alg:gen_variable_fixing}
\begin{algorithmic}[1]
\Require{Fraction of variables to fix $\rho$, $0 < \rho < 1$}
\Ensure{Set of integer indices $F$}
\Function{RandomFixings}{$\rho$}
    \State $i :=$ random element in $\{1\dots n\}$ \Comment $n$: number of variables in the original MIP 
    \State $F :=$ first $\rho \cdot n$ consecutive \cancel{integer} variable indices starting from $i$ in a circular fashion
    \State \Return $F$
\EndFunction
\end{algorithmic}
\end{algorithm}
Since these algorithms are executed on the auxiliary MIP problems -FMIP or OMIP-, the variables subject to fixing correspond exactly to those defined in the original MIP formulation.  
By generalizing the fixing strategy to include both integer and continuous variables, diversification is enhanced, thereby reducing the likelihood of stagnation in local minima and potentially improving the exploration of the solution space.

\section{Architecture-Agnostic Parallelization}
In the original study, the Message Passing Interface (MPI) was employed to synchronize processors at each recombination phase. While this approach is well suited to high-performance computing environments, it may be unnecessarily complex in general-purpose scenarios, where a simpler multi-threading implementation is often preferable.  
In this thesis, communication is instead managed through a set of logical threads, which may differ from the number of available hardware threads. This abstraction ensures that, even on machines with fewer physical cores, the algorithm can reproduce the same behavior across different architectures, provided sufficient computational time is allowed.  
More specifically, during the coordination phase, either the most feasible or the most optimal solution—depending on whether a recombination FMIP or OMIP is performed—is shared among the logical processors. Each processor then continues working independently on its own copy, with updates to the incumbent solution handled exclusively through a thread-safe update function, formally described in Algorithm~\ref{alg:inc_update}.  
\begin{algorithm}[H]
\caption{Parallel ACS Incumbent Update Procedure}\label{alg:inc_update}
\begin{algorithmic}[1]
\Require Candidate solution $x$ with slack sum $S(x)$ and objective value $C(x)$; Incumbent $\tilde{x}$; Zero-tolerance $\epsilon$
\Ensure Updated incumbent $\tilde{x}$ in a thread-safe manner
\Function{UpdateIncumbent}{$x$}
    \State acquire lock
    \If{$(|S(x)| < |S(\tilde{x})|) \;\;\lor\;\; (|S(x)| < \epsilon \;\land\; C(x) < C(\tilde{x}))$}
        \State $\tilde{x} \gets x$
    \EndIf
    \State release lock
\EndFunction
\end{algorithmic}
\end{algorithm}

This mechanism is crucial to ensure that the algorithm consistently improves and converges within the given time limit. The incumbent solution is updated whenever a better solution is identified: either one with a smaller slack sum, indicating improved feasibility, or one with a lower objective value $c^T x$ provided that the slack sum is less than the tolerance parameter $\epsilon$, the zero-feasibility threshold.

\section{Eliminating Calibration in PACS Parameter Selection}
After adapting the PACS algorithm to a more general-purpose environment, another important challenge arises: parameter selection. Since PACS must be applicable to a wide range of hard MIP instances, it is necessary to identify parameter settings that generally perform well, both in terms of computational efficiency and heuristic solution quality.  
The goal is to remove the need for an explicit calibration phase, while still ensuring reliable performance. The tuning process specifically concerns the following parameters:
\begin{enumerate}
    \item The time span assigned to each sub-MIP, either FMIP\_LNS or OMIP\_LNS
    \item The parameter $\rho$ governing the fixing strategy 
    \item The parameter $\theta$ governing the starting vector
\end{enumerate}

\subsection{Adaptive Determination of Sub-MIP Time Span}
To guarantee determinism in the implementation, each sub-MIP is assigned both a time limit equal to the remaining computation time and a deterministic time limit. The latter is defined as the maximum number of instructions that the solver may execute before termination.  
The deterministic time limit is computed according to the following formula:
\begin{equation}
TL_{DET} = \max\Big(x, \min\Big(\frac{nz}{y}, X\Big)\Big)
\end{equation}
which provides a dynamic mechanism for adapting the computational effort of each sub-MIP.  
In this formulation, $x$ and $X$ denote the minimum and maximum allowable deterministic time limits, respectively, $nz$ represents the number of nonzeros in the constraint matrix $A$ of the MIP problem, and $y$ acts as a scaling factor. The chosen parameter values are:
\begin{enumerate}
\item Minimum deterministic time limit: $x = 10^3$  
\item Scaling factor: $y = 10^2$  
\item Maximum deterministic time limit: $X = 10^7$  
\end{enumerate}
This dynamic adjustment removes the need to explicitly set a fixed time limit for each subproblem, thereby eliminating the necessity of parameter tuning in this respect.

\subsection{Adaptive Variable Fixing through $\rho$ Adjustment}\label{sec:dyn_var_fix}
\subsubsection{Fixed $\rho$ Initialization}
As discussed in Section \ref{sec:PACS_var_fix}, selecting an appropriate variable fixing scheme is a non-trivial task. In particular, the random fixings scheme presents additional challenges: if the fraction $\rho$ of variables to be fixed is set too high, the procedure may fail to yield meaningful improvements; conversely, if $\rho$ is set too low, the resulting search space may become excessively large, making it computationally intractable within a reasonable time frame. \\
For this reason, in the first instance, the parameter $\rho$ is selected from a set of predetermined candidate values. Anticipating the results presented in Section \ref{}, it can be observed that certain values of $\rho$ are more effective in terms of solution quality, as they grant the solver greater flexibility during the optimization process.
\subsubsection{Dynamic Adjustment of Parameter $\rho$}
Since the LNS heuristics in the PACS algorthm restrict the search space by fixing a number of variables according to the value of $\rho$, it is natural to design a mechanism for dynamically adapting this parameter in order to increase the likelihood of discovering high-quality solutions.  
\begin{algorithm}[H]
\caption{Parallel ACS Rho Update (Parallel Phases)}\label{alg:rho_update_MT}
\begin{algorithmic}[1]
\Require Status code $MIP_{code}$ returned by the solver; Adjustment step $\Delta_\rho$ for $\rho$; The variable fixing parameter $\rho$; Number of parallel sub-MIPs $num_{MIP}$
\Ensure Updated value of $\rho$, synchronized across parallel sub-MIPs
\Function{DynRhoUpdate}{$MIP_{code}$, $\Delta_\rho$, $\rho$, $num_{MIP}$}
    \State $\hat\Delta_\rho \gets {\Delta_\rho \over{num_{MIP}}}$
    \State acquire lock
    \If{$MIP_{code} = OPT \;\lor\; MIP_{code} = OPT_{TOL}$}
        \State $\rho \gets \rho - \hat\Delta_\rho$
    \EndIf
    \If{$MIP_{code} = FEAS_{TL} \;\lor\; MIP_{code} = FEAS_{DET\_TL}$}
        \State $\rho \gets \rho + \hat\Delta_\rho$
    \EndIf
    \State Clap $\rho$ within $[0.01,0.99]$
    \If{*Tie Case detected*}
        \State $\rho \gets \rho - \Delta_\rho$
    \EndIf
    \State release lock
\EndFunction
\end{algorithmic}
\end{algorithm}
The procedure, described in Algorithm \ref{alg:rho_update_MT}, is applied after each sub-MIP optimization phase in the parallel step. Based on the status returned by the solver, the value of $\rho$ is updated as follows:
\begin{itemize}
    \item  If the solver hits the time limit—either the deterministic limit or the global remaining time—while still producing a feasible solution, $\rho$ is increased by $\frac{\Delta_\rho}{num_{MIP}}$. This adjustment suggests fixing more variables in subsequent phases, thereby simplifying the subproblem to be solved. 
    \item Conversely, if the solver converges to an optimal solution within the tolerance, this indicates that the corresponding region of the search space has already been sufficiently explored. In this case, $\rho$ is decreased by $\frac{\Delta_\rho}{num_{MIP}}$, enlarging the search space and granting the solver greater freedom in the following optimization steps.  
\end{itemize}
Since each sub-MIP independently attempts to modify the value of $\rho$, synchronization through locking is required to prevent inconsistencies. The final update is determined as the average of the adjustments proposed by the parallel optimization phases. In case of a tie, a deterministic rule is applied: $\rho$ is decreased by $\Delta_\rho$.  
\begin{algorithm}[H]
\caption{Parallel ACS Rho Update (Recombination Phases)}\label{alg:rho_update}
\begin{algorithmic}[1]
\Require Status code $MIP_{code}$ returned by the solver; Adjustment step $\Delta_\rho$ for $\rho$; The variable fixing parameter $\rho$; Number of parallel sub-MIPs $num_{MIP}$
\Ensure Updated value of $\rho$ after recombination adjustment
\Function{DynRhoUpdate}{$MIP_{code}$, $\Delta_\rho$, $\rho$, $num_{MIP}$}
    \State $\hat\Delta_\rho \gets {2\Delta_\rho \over{num_{MIP}}}$
    \If{$MIP_{code} = OPT \;\lor\; MIP_{code} = OPT_{TOL}$}
        \State $\rho \gets \rho - \hat\Delta_\rho$
    \EndIf
    \If{$MIP_{code} = FEAS_{TL} \;\lor\; MIP_{code} = FEAS_{DET\_TL}$}
        \State $\rho \gets \rho + \hat\Delta_\rho$
    \EndIf
    \State Clap $\rho$ within $[0.01,0.99]$
\EndFunction
\end{algorithmic}
\end{algorithm}
For the recombination phase, the procedure is slightly modified into Algorithm \ref{alg:rho_update}, where the adjustment step is doubled, i.e. $2\Delta_\rho / num_{MIP}$, in order to resolve potential tie cases.\\
Finally, although an initial value of $\rho$ must be provided to start the fixing process, experimental results in Section \ref{} show that performance is only marginally affected by this initialization. Consequently, the effectiveness of the method does not critically depend on the initial choice of $\rho$.

\subsection{Parameter-Free Strategy for Initial Solution Construction}
Another non-trivial challenge concerns the selection of the initial solution vector. The parameter $\theta$, which regulates the trade-off between solution quality and execution time, must be tuned to suit the most general cases. Similar to the fixing strategy, the analysis considers a set of predetermined values for $\theta$, while also introducing a novel initialization strategy for the starting solution vector.  
The motivation is that using a small value of $\theta$—which corresponds to generating a high-quality initial solution—may waste computation time, since the true strength of PACS lies in its diversification and parallelization capabilities.  

Therefore, Algorithm \ref{alg:max_feasSol} is designed with the following goal: to provide a heuristic starting solution that is deterministic and computationally lightweight, while remaining reasonably feasible for the original MIP problem.  
\begin{algorithm}[H]
\caption{Heuristic Initialization of Starting Solution}\label{alg:max_feasSol}
\begin{algorithmic}[1]
\Require $MIP$ instance with $n$ variables, objective coefficients $c_i$, bounds $[l_i, u_i]$; zero-tolerance $\epsilon$
\Ensure Starting solution vector $x \in \bar{\mathbb{R}}^n$
\Function{StartSolMaxFeas}{$MIP$}
    \State $x \gets ( +\infty, \ldots, +\infty ) \in \bar{\mathbb{R}}^n$
    \For{$i = 1 \to n$}
        \State $(l_i, u_i) \gets$ variable bounds of variable $i$
        \If{$l_i = -\infty \;\land\; u_i = +\infty$}
            \State $x_i \gets 0$
        \ElsIf{$l_i > -\infty \;\land\; u_i = +\infty$}
            \State $x_i \gets l_i$
        \ElsIf{$l_i = -\infty \;\land\; u_i < +\infty$}
            \State $x_i \gets u_i$
        \Else
            \If{$c_i \leq -\epsilon$}
                \State $x_i \gets l_i$
            \ElsIf{$c_i \geq \epsilon$}
                \State $x_i \gets u_i$
            \Else
                \State $x_i \gets$ \Call{RandomInteger}{$l_i, u_i$}
            \EndIf
        \EndIf
    \EndFor
    \State \Return $x$
\EndFunction
\end{algorithmic}
\end{algorithm}
The initialization procedure in Algorithm \ref{alg:max_feasSol} operates as follows:
\begin{itemize}
    \item If a variable has no finite bounds, it is set to $0$.
    \item If a variable has only one finite bound, it is set to that bound.
    \item If both bounds are finite:
    \begin{itemize}
        \item set the variable to the lower bound if $c_i \leq -\epsilon$,i.e. the coefficient is negative,
        \item set it to the upper bound if $c_i \geq \epsilon$, i.e.the coefficient is positive,
        \item choose a random integer within $[l_i, u_i]$ if $-\epsilon \leq c_i \leq \epsilon$, i.e.the coefficient is close to zero.
    \end{itemize}
\end{itemize}
In this way, the solution initialization is almost entirely deterministic and computationally lightweight, although feasibility with respect to the original MIP problem may be partially compromised. Furthermore, the proposed strategy is parameter-free, which makes it particularly well-suited for the purposes of this thesis.

\section{Additional PACS Optimizations}
While the previous modifications focused on tuning or slightly adjusting specific components of the PACS workflow, the following procedures—analyzed in Section \ref{}—aim to further enhance the quality of the final solution.

\subsection{Slack Upper Bound Enforcement and Budget Constraint Removal}
The first idea is to exploit the values of the slack variables $\Delta^+$ and $\Delta^-$ obtained in a previous iteration to enforce these values as upper bounds.  
In the PACS algorithm, this mechanism is already embedded in the budget constraint $\sum_{i=0}^m \Delta_i^{+}+\Delta_i^{-} \le \sum_{i=0}^m \hat\Delta_i^{+}+\hat\Delta_i^{-}$, which provides only a global upper bound. The proposed strategy, instead, aims at fixing the upper bounds of individual slack variables, thereby assisting subsequent FMIP optimizations and improving feasibility.  
The procedure is implemented as shown in Algorithm \ref{alg:fixing_slackUB}.  
\begin{algorithm}[H]
\caption{Fixing Slack Variables to Upper Bound}\label{alg:fixing_slackUB}
\begin{algorithmic}[1]
\Require $MIP$ model with $n$ variables; Solution $x$ of length $n+2m$; Zero-tolerance $\epsilon$
\Ensure Updated model with adjusted upper bounds for slack variables
\Function{SlackUpperBoundFixing}{$MIP$, $x$}
    \State $S_{UB} \gets \emptyset$ \Comment Set of variables-upper bound to be updated
    \For{$i = n \to n+2m$}
        \State $u_i \gets$ upper bound of variable $i$
        \If{$u_i - x_i > \epsilon$}
            \State $S_{UB} \gets  S_{UB} \cup (i,x_i)$
        \EndIf
    \EndFor
    \State \Return $S_{UB}$
\EndFunction
\end{algorithmic}
\end{algorithm}
Here, the solution vector $x$ corresponds to the current PACS incumbent—valid in both the parallel and recombination phases—and each slack variable is compared with its respective upper bound. If the variable value is smaller than the existing upper bound, the variable index together with its solution value is added to $S_{UB}$ as a pair $(i, x_i)$.
The resulting set $S_{UB}$ is subsequently employed to update the upper bounds of the slack variables, thereby modifying the auxiliary FMIP and OMIP formulations as follows.

\subsubsection{FMIP Reformulation}
\begin{equation}
\begin{cases}
\text{min} \quad & \sum_{i=0}^m \Delta_i^{+}+\Delta_i^{-} \\ 
\text{s.t.} \quad & Ax + I_m\Delta^+ - I_m\Delta^- = b\\ 
& x_i = \hat{x_i}, \; \forall i \in F\\ 
& \Delta^+_i \le \hat{u}_i, \; \forall (i,\hat{u}_i) \in S_{UB}^+\\
& \Delta^-_i \le \hat{u}_i, \; \forall (i,\hat{u}_i) \in S_{UB}^-\\  
& l \le x \le u\\ 
& x_i \in \mathbb{Z}, \; \forall i \in \mathcal{I} \\ 
& \Delta^+ \ge 0, \; \Delta^- \ge 0
\end{cases}
\end{equation}

\subsubsection{OMIP Reformulation}
\begin{equation}
\begin{cases}
\text{min} \quad & c^T x \\ 
\text{s.t.} \quad & Ax + I_m\Delta^+ - I_m\Delta^- = b\\
& \cancel{\sum_{i=0}^m \Delta_i^{+}+\Delta_i^{-} \le \sum_{i=0}^m \hat\Delta_i^{+}+\hat\Delta_i^{-}}\\ 
& x_i = \hat{x}_i, \; \forall i \in F\\ 
& \Delta^+_i \le \hat{u}_i, \; \forall (i,\hat{u}_i) \in S_{UB}^+\\
& \Delta^-_i \le \hat{u}_i, \; \forall (i,\hat{u}_i) \in S_{UB}^-\\  
& l \le x \le u\\ 
& x_i \in \mathbb{Z}, \; \forall i \in \mathcal{I} \\ 
& \Delta^+ \ge 0, \; \Delta^- \ge 0 
\end{cases}
\end{equation}
In these formulations, the set $S_{UB}$ is partitioned into $S_{UB}^+$ and $S_{UB}^-$ to remain consistent with the notation used for FMIP and OMIP.  
It is worth noting that the budget constraint in OMIP can now be removed, since the slack upper bound fixing provides a stronger restriction, as will be further confirmed by the results in Section \ref{}.  

\subsection{WalkMIP: Violated-Constraints-Based Variable Fixing}

It is worthwhile to investigate a more sophisticated and potentially efficient variable fixing strategy, aiming to enrich diversification and consequently increase the likelihood of finding high-quality solutions within a relatively short time span. The WalkMIP strategy adapts ideas from the WalkSAT$^\text{\cite{WalkSAT}}$ algorithm, targeting specifically the variables that cause infeasibility in a partially feasible solution. 

Algorithm \ref{alg:walkMIP} presents the WalkMIP procedure adapted for MIP constraint satisfaction problems.

\begin{algorithm}[H]
\caption{Walk-based Repair Heuristic for MIP}\label{alg:walkMIP}
\begin{algorithmic}[1]
\Require $MIP$ model with $n+2m$ variables (FMIP or OMIP); Initial solution $x$; Fixing parameter $\rho$; Random walk probability $p \in (0,1)$; Solution kick probability $p_{HUGE\_KICK}$
\Ensure Updated model with improved feasible solution
\Function{WalkMIP}{$MIP$, $x$, $\rho$, $p$,$p_{HUGE\_KICK}$}
    \State $violConstr \gets$ Set of indices of violated constraints, under $x$
    \If{*First Execution*} $numInitialConstr \gets |violConstr|$\EndIf
    \If{$violConstr = \emptyset$ } \Return \EndIf
    \If{with probability $p_{HUGE\_KICK}$} 
        \State \Call{RandomFixings}{$\rho$}
    \EndIf
    \State $k \gets $\Call{NumConstrToFix}{$numInitialConstr$,$|violConstr|$}
    \Repeat
        \State $c \gets $ Random violated constraint, $c \in violConstr$
        \State $(i_{best}, val_{best}, minDMG) \gets$ \Call{MinDamageMove}{$c, x$}
        \If{$minDMG \leq -\epsilon$}
            \State $\hat{x} \gets \Call{UpdateSol}{x,i_{best},val_{best}}$\Comment Apply best move to $x$
        \ElsIf{with probability $1-p \land i_{best} \neq -1$}
             \State $\hat{x} \gets \Call{UpdateSol}{x,i_{best},val_{best}}$\Comment Apply min-damage move to $x$
        \Else 
            \State $i_{rnd} \gets \Call{RandomInteger}{0,n-1}$
            \State $\hat{x} \gets \Call{UpdateSol}{x, i_{rnd},x_{i_{rnd}}}$ \Comment Randomly fixing a var in $x$
        \EndIf
        \State $violConstr \gets$ Set of indices of violated constraints, under $\hat{x}$
        \If{$violConstr = \emptyset$} \textbf{break} \EndIf
    \Until $k$ times

    \If{$k < \rho \cdot n$}
        \State $\hat{x} \gets$Fix remaining variables (up to $\rho \cdot n$) from original $x$
    \EndIf
    \Return $\hat{x}$
\EndFunction
\end{algorithmic}
\end{algorithm}

The WalkMIP algorithm is executed independently by each processor, replacing the standard random fixing procedure. The strategy can be decomposed into three main phases:

\begin{enumerate}
    \item \textbf{Initialization and Huge-Kick Phase (lines 1-9 in Algorithm \ref{alg:walkMIP})}: If this is the first execution of the routine, the algorithm records the number of violated constraints under the current solution $x$, which is later used to determine the number of iterations for the main loop. To enhance diversification and escape local minima, a probability $p_{HUGE\_KICK}$ is introduced to randomly fix variables according to $\rho$. In our implementation, this parameter is set to $1/16$.
    
    \item \textbf{Constraint Selection and Optimization Loop (lines 10-25 in Algorithm \ref{alg:walkMIP})}: The number of constraints to target is determined by the function:
    $$
    k(v, v_0) =
    \begin{cases}
    \left\lceil k_{\min} + (k_{\max}-k_{\min}) \cdot \hat{r}^{\,\beta} \right\rceil & \text{if } v > k_{\min} \\[0.5em]
    v & \text{otherwise}
    \end{cases}
    $$
    where
    $$
    \hat{r} = \min\!\left(1, \max\!\left(0, \frac{v}{\max(1, v_0)}\right)\right),\quad k_{\min}=32,\; k_{\max}=1024,\; \beta=\frac{\sqrt{5}-1}{2}.
    $$
    This ensures that the number of touched constraints is bounded by $k_{\max}$ while accounting for constraints that have become feasible since the algorithm started. Only one variable per constraint is modified, imitating the WalkSAT strategy. The main loop iterates over $k$ randomly chosen violated constraints---updated at each iteration---, selecting the "minimum damage move" for each constraint. The function \textsc{MinDamageMove}, the Algorithm \ref{alg:min_DMG_WalkMIP}, evaluates potential moves:  
        \begin{itemize}
            \item If a variable fixing reduces infeasibility, the best move is applied.
            \item Otherwise, with probability $1-p$, the minimal-damage move is applied.
            \item Otherwise, with probability $p$, a random variable is fixed using its current solution value $x_{i_{rnd}}$.
        \end{itemize}
    
    \item \textbf{Fixing Remaining Variables (lines 26-29 in Algorithm \ref{alg:min_DMG_WalkMIP})}: To maintain consistency with the original variable fixing strategy, the total number of fixed variables is set to $\rho \cdot n$. If the previous phases fixed fewer variables, the remaining variables are fixed according to the standard strategy.
\end{enumerate}

\begin{algorithm}[H]
\caption{Minimum Damage Move Selection for a Violated Constraint in WalkMIP}
\begin{algorithmic}
\Require Violated constraint $c$, current solution vector $x$
\Ensure Tuple $(i_{best}, val_{best}, minDMG)$, where $i_{best}$ is the index of the variable to modify, $val_{best}$ is the new value for that variable, and $minDMG$ is the resulting minimum change in constraint violation
\Function{MinDamageMove}{$c, x$}\label{alg:min_DMG_WalkMIP}
    \State $minDMG \gets +\infty$, $i_{best} \gets -1$, $val_{best} \gets +\infty$
    \For{\textbf{each} variable $i$ in $c$}
        \If{$i$.type = BIN} $\hat{x}_i \gets \neg x_i$
        \ElsIf{$i$.type = INT} $\hat{x}_i \gets x_i + \Call{RandomInteger}{-1,1}$
        \ElsIf{$i$.type = DOUBLE} $\hat{x}_i \gets x_i + \Call{RandomDouble}{-0.5,0.5}$
        \EndIf
        \State Clamp $\hat{x}_i$ within its bounds $[l_i,u_i]$
        \State $\delta \gets \Call{ComputeViolation}{x,\hat{x}_i}$
        \If{$\delta < minDMG$}
            \State $i_{best} \gets i$, $minDMG \gets \delta$, $val_{best} \gets \hat{x}_i$
        \EndIf
    \EndFor
    \State \Return $(i_{best}, val_{best}, minDMG)$
\EndFunction
\end{algorithmic}
\end{algorithm}

Notice that any modifications of $\rho$ via the dynamic adjustment routine are incorporated in subsequent WalkMIP iterations. It is important to emphasize that WalkMIP can only be applied when the slack variables are nonzero. Consequently, the original random fixing strategy, described in Section \ref{sec:dyn_var_fix}, remains necessary during OMIP optimization phases in which the slack sum is zero.
Although the results in Section \ref{} indicate that this strategy is less effective than the simpler, more straightforward variable fixing approach, it offers valuable insights and suggests a potential direction for future research.