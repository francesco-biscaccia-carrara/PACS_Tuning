According to the original study, although the baseline PACS algorithm demonstrates strong performance on many MIP instances, its exclusive focus on integer variables and dependence on fixed parameters can restrict diversification and increase the risk of premature convergence.

This section presents enhancements aimed at overcoming these limitations:
\begin{enumerate}
\item Generalizing starting vector construction and variable fixing to include both integer and continuous variables.
\item Implementing architecture-agnostic parallelization for consistent behavior across hardware.
\item Removing manual parameter tuning via adaptive sub-MIP time limits and dynamic adjustment of $\rho$.
\item Introducing a parameter-free initial solution strategy.
\item Adding optimization procedures, including slack upper bound enforcement and WalkMIP-based variable fixing, to improve feasibility and solution quality.
\end{enumerate}

These improvements increase PACS’s robustness, efficiency, and applicability while preserving its strengths in diversification and parallelism.

\section{PACS with Generalized Fixing}\label{sec:gen_fixing}
The baseline PACS algorithm enforces both the starting vector construction and the fixing scheme to operate exclusively on the set $\mathcal{I}$ of integer variables. While this restriction may appear efficient and straightforward, it can, in fact, be limiting. In particular, if the MIP instance contains only a small fraction of integer variables relative to the total, focusing solely on them may reduce diversification and cause the algorithm to converge prematurely to a local minimum, which is undesirable in an optimization process.
To overcome this issue, Algorithm \ref{alg:starting_vector} and Algorithm \ref{alg:variable_fixing} are generalized into Algorithm \ref{alg:gen_starting_vector} and Algorithm \ref{alg:gen_variable_fixing}, respectively, thereby allowing both integer and continuous variables to be considered.
\begin{algorithm}[H]
\caption{Generalized Starting vector heuristic}\label{alg:gen_starting_vector}
\begin{algorithmic}[1]
\Require{Original $MIP$ formulation; Percentage of variables to fix $\theta \in (0,100]$; Fixed bound constant $c_b$}
\Ensure{Starting integer-feasible vector $\hat{x}$}
\Function{InitThetaSolution}{$MIP$, $\theta$, $c_b$}
\State $V \gets \text{sort}_\uparrow(\big \{x \in \{x_1,\dots,x_n\}\big\}, (u_x-l_x))$
\State $F \gets \emptyset$
\While{$\lnot \Call{IsIntegerFeasible}{\hat{x}} \land |F| \neq |V|$}
    \State $\mathcal{K} \gets$ top $\theta \%$ of unfixed variables from $V$
    \For{$k \in \mathcal{K}$}
        \State $\hat{x}_k \gets \Call{RandomInteger}{\max(l_k, -c_b), \min(u_k, c_b)}$
    \EndFor
    \State $F \gets F \cup \mathcal{K}$
    \State $[x, \Delta^+, \Delta^-] \gets \Call{RELAXED\_MIP}{MIP, F, \hat{x}}$
    \State $Q \gets \{i \;|\; x_i \in x, x_i \in \mathbb{Z},\cancel{x_i \in \mathcal{I}}\}$
    \State $\hat{x}_q \gets x_q, \; \forall q \in Q$
    \State $F \gets F \cup Q$
\EndWhile
\State \Return $\hat{x}$
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Generalized Variable Fixing Selection Algorithm}\label{alg:gen_variable_fixing}
\begin{algorithmic}[1]
\Require{Original $MIP$ formulation; Fraction of variables to fix $\rho \in (0,1)$}
\Ensure{Set of integer indices $F$}
\Function{RandomFixings}{$MIP$, $\rho$}
    \State $i \gets \Call{UniformFrom}{\{1,\dots,n\}}$ \Comment $n$: number of variables in the original MIP 
    \State $F \gets \big[(i,i+1,\dots,i+\rho\cdot n -1) \mod |n|\big]$
    \State \Return $F$
\EndFunction
\end{algorithmic}
\end{algorithm}
Although these algorithms operate on the auxiliary MIP problems---FMIP or OMIP---the variables subject to fixing are the same as those in the original MIP formulation. By extending the fixing strategy to include both integer and continuous variables, diversification is improved, reducing the likelihood of stagnation in local minima and enhancing exploration of the solution space.
\section{Architecture-Agnostic Parallelization}
In the original study, the Message Passing Interface (MPI) was employed to synchronize processors at each recombination phase. While this approach is well suited to high-performance computing environments, it may be unnecessarily complex in general-purpose scenarios, where a simpler multi-threading implementation is often preferable.  
In this thesis, communication is instead managed through a set of logical threads, which may differ from the number of available hardware threads. This abstraction ensures that, even on machines with fewer physical cores, the algorithm can reproduce the same behavior across different architectures, provided sufficient computational time is allowed.  
More specifically, during the coordination phase, either the most feasible or the most optimal solution—depending on whether a recombination FMIP or OMIP is performed—is shared among the logical processors. Each processor then continues working independently on its own copy, with updates to the incumbent solution handled exclusively through a thread-safe update function, formally described in Algorithm~\ref{alg:inc_update}.  
\begin{algorithm}[H]
\caption{Parallel ACS Incumbent Update Procedure}\label{alg:inc_update}
\begin{algorithmic}[1]
\Require Original $MIP$ formulation; Candidate solution $[\hat{x}, \hat{\Delta}^+, \hat{\Delta}^-]$; Incumbent $x_{ACS}$ with slack sum $\Delta_{ACS}$ and objective value $C_{ACS}=c^T x_{ACS}$; Zero-tolerance $\epsilon$
\Ensure Updated incumbent $x_{ACS}$ in a thread-safe manner
\Function{UpdateIncumbent}{$MIP$, $[\hat{x}, \hat{\Delta}^+, \hat{\Delta}^-]$}
    \State \textbf{acquire lock}
    \If{$(|\sum_i^m \hat{\Delta}_i^+ + \hat{\Delta}_i^- | < |\Delta_{ACS}|) \;\;\lor\;\; (|\sum_i^m \hat{\Delta}_i^+ + \hat{\Delta}_i^- | < \epsilon \;\land\; c^T \hat{x} < C_{ACS})$}
        \State $[x_{ACS},\Delta_{ACS},C_{ACS}] \gets [\hat{x},\sum_i^m \hat{\Delta}_i^+ + \hat{\Delta}_i^-,c^T \hat{x}]$
    \EndIf
    \State \textbf{release lock}
\EndFunction
\end{algorithmic}
\end{algorithm}

This mechanism is crucial to ensure that the algorithm consistently improves and converges within the given time limit. The incumbent solution is updated whenever a better solution is identified: either one with a smaller slack sum, indicating improved feasibility, or one with a lower objective value $c^T x$ provided that the slack sum is less than the tolerance parameter $\epsilon$, the zero-feasibility threshold.

\section{Eliminating Calibration in PACS Parameter Selection}
After adapting the PACS algorithm to a more general-purpose environment, another important challenge arises: parameter selection. Since PACS must be applicable to a wide range of hard MIP instances, it is necessary to identify parameter settings that generally perform well, both in terms of computational efficiency and heuristic solution quality.  
The goal is to remove the need for an explicit calibration phase, while still ensuring reliable performance. The tuning process specifically concerns the following parameters:
\begin{enumerate}
    \item The time span assigned to each sub-MIP, either \textsc{FMIP\_LNS} or \textsc{OMIP\_LNS}
    \item The parameter $\rho$ governing the fixing strategy 
    \item The parameter $\theta$ governing the starting vector
\end{enumerate}

\subsection{Adaptive Determination of Sub-MIP Time Span}
To guarantee determinism in the implementation, each sub-MIP is assigned both a time limit equal to the remaining computation time and a deterministic time limit. The latter is defined as the maximum number of instructions that the solver may execute before termination.  
The deterministic time limit is computed according to the following formula$^\text{\cite{Piai}}$:
\begin{equation}
TL_{DET} = \max\Big(x, \min\Big(\frac{nz}{y}, X\Big)\Big)
\end{equation}
which provides a dynamic mechanism for adapting the computational effort of each sub-MIP.  
In this formulation, $x$ and $X$ denote the minimum and maximum allowable deterministic time limits, respectively, $nz$ represents the number of nonzeros in the constraint matrix $A$ of the MIP problem, and $y$ acts as a scaling factor. The chosen parameter values are:
\begin{enumerate}
\item Minimum deterministic time limit: $x = 10^3$  
\item Scaling factor: $y = 10^2$  
\item Maximum deterministic time limit: $X = 10^7$  
\end{enumerate}
This dynamic adjustment removes the need to explicitly set a fixed time limit for each subproblem, thereby eliminating the necessity of parameter tuning in this respect.

\subsection{Adaptive Variable Fixing through $\rho$ Adjustment}\label{sec:dyn_var_fix}
\subsubsection{Fixed $\rho$ Initialization}
As discussed in Section \ref{sec:PACS_var_fix}, selecting an appropriate variable fixing scheme is a non-trivial task. In particular, the random fixings scheme presents additional challenges: if the fraction $\rho$ of variables to be fixed is set too high, the procedure may fail to yield meaningful improvements; conversely, if $\rho$ is set too low, the resulting search space may become excessively large, making it computationally intractable within a reasonable time frame. \\
For this reason, in the first instance, the parameter $\rho$ is selected from a set of predetermined candidate values. Anticipating the results presented in Section \ref{sec:test_fix_rho}, it can be observed that certain values of $\rho$ are more effective in terms of solution quality, as they grant the solver greater flexibility during the optimization process.
\subsubsection{Dynamic Adjustment of Parameter $\rho$}
Since the LNS heuristics in the PACS algorthm restrict the search space by fixing a number of variables according to the value of $\rho$, it is natural to design a mechanism for dynamically adapting this parameter in order to increase the likelihood of discovering high-quality solutions.  
\begin{algorithm}[H]
\caption{Parallel ACS Rho Update (Parallel Phases)}\label{alg:rho_update_MT}
\begin{algorithmic}[1]
\Require Status code $MIP_{code}$ returned by the solver; Adjustment step $\delta_\rho$ for $\rho$; The variable fixing parameter $\rho$; Number of parallel sub-MIPs $num_{MIP}$
\Ensure Updated value of $\rho$, synchronized across parallel sub-MIPs
\Function{DynRhoUpdate}{$MIP_{code}$, $\delta_\rho$, $\rho$, $num_{MIP}$}
    \State $\hat\delta_\rho \gets {\delta_\rho \over{num_{MIP}}}$
    \State \textbf{acquire lock}
    \If{$MIP_{code} = OPT \;\lor\; MIP_{code} = OPT_{TOL}$}
        \State $\rho \gets \rho - \hat\delta_\rho$
    \EndIf
    \If{$MIP_{code} = FEAS_{TL} \;\lor\; MIP_{code} = FEAS_{DET\_TL}$}
        \State $\rho \gets \rho + \hat\delta_\rho$
    \EndIf
    \State $\rho \gets \Call{Clamp}{\rho,0.01,0.99}$
    \If{*Tie Case detected*}
        \State $\rho \gets \rho - \delta_\rho$
    \EndIf
    \State \textbf{release lock}
\EndFunction
\end{algorithmic}
\end{algorithm}
The procedure, described in Algorithm \ref{alg:rho_update_MT}, is applied after each sub-MIP optimization phase in the parallel step. Based on the status returned by the solver, the value of $\rho$ is updated as follows:
\begin{itemize}
    \item  If the solver hits the time limit—either the deterministic limit or the global remaining time—while still producing a feasible solution, $\rho$ is increased by $\frac{\delta_\rho}{num_{MIP}}$. This adjustment suggests fixing more variables in subsequent phases, thereby simplifying the subproblem to be solved. 
    \item Conversely, if the solver converges to an optimal solution within the tolerance, this indicates that the corresponding region of the search space has already been sufficiently explored. In this case, $\rho$ is decreased by $\frac{\delta_\rho}{num_{MIP}}$, enlarging the search space and granting the solver greater freedom in the following optimization steps.  
\end{itemize}
Since each sub-MIP independently attempts to modify the value of $\rho$, synchronization through locking is required to prevent inconsistencies. The final value of the parameter $\rho$ is determined as the average of the adjustments proposed by the parallel optimization phases. In case of a tie, a deterministic rule is applied: $\rho$ is decreased by $\delta_\rho$.  
\begin{algorithm}[H]
\caption{Parallel ACS Rho Update (Recombination Phases)}\label{alg:rho_update}
\begin{algorithmic}[1]
\Require Status code $MIP_{code}$ returned by the solver; Adjustment step $\delta_\rho$ for $\rho$; The variable fixing parameter $\rho$; Number of parallel sub-MIPs $num_{MIP}$
\Ensure Updated value of $\rho$ after recombination adjustment
\Function{DynRhoUpdate}{$MIP_{code}$, $\delta_\rho$, $\rho$, $num_{MIP}$}
    \State $\hat\delta_\rho \gets {2\delta_\rho \over{num_{MIP}}}$
    \If{$MIP_{code} = OPT \;\lor\; MIP_{code} = OPT_{TOL}$}
        \State $\rho \gets \rho - \hat\delta_\rho$
    \EndIf
    \If{$MIP_{code} = FEAS_{TL} \;\lor\; MIP_{code} = FEAS_{DET\_TL}$}
        \State $\rho \gets \rho + \hat\delta_\rho$
    \EndIf
    \State $\rho \gets \Call{Clamp}{\rho,0.01,0.99}$
\EndFunction
\end{algorithmic}
\end{algorithm}
For the recombination phase, the procedure is slightly modified into Algorithm \ref{alg:rho_update}, where the adjustment step is doubled, i.e. $2\delta_\rho / num_{MIP}$, in order to resolve potential tie cases.\\
Finally, although an initial value of $\rho$ must be provided to start the fixing process, experimental results in Section \ref{sec:test_dyn_rho} show that performance is only marginally affected by this initialization. Consequently, the effectiveness of the method does not critically depend on the initial choice of $\rho$.

\subsection{Parameter-Free Strategy for Initial Solution Construction}\label{sec:init_sol_maxFeas}
Another non-trivial challenge concerns the selection of the initial solution vector. The parameter $\theta$, which regulates the trade-off between solution quality and execution time, must be tuned to suit the most general cases. Similar to the fixing strategy, the analysis considers a set of predetermined values for $\theta$, while also introducing a novel initialization strategy for the starting solution vector.  
The motivation is that using a small value of $\theta$---which corresponds to generating a high-quality initial solution---may waste computation time, since the true strength of PACS lies in its diversification and parallelization capabilities.  

Therefore, Algorithm \ref{alg:max_feasSol} is designed with the following goal: to provide a heuristic starting solution that is deterministic and computationally lightweight, while remaining reasonably feasible for the original MIP problem.  
\begin{algorithm}[H]
\caption{Heuristic Initialization of Starting Solution}\label{alg:max_feasSol}
\begin{algorithmic}[1]
\Require Original $MIP$ formulation with $n$ variables, objective coefficients $c_i$, bounds $[l_i, u_i]$; Zero-tolerance $\epsilon$
\Ensure Starting solution vector $\hat{x} \in \bar{\mathbb{R}}^n$
\Function{StartSolMaxFeas}{$MIP$}
    \State $[\hat{x},\hat\Delta^+,\hat\Delta^-] \gets ( +\infty, \ldots, +\infty ) \in \bar{\mathbb{R}}^{n+2m}$
    \For{$i = 1 \to n$}
        \State $(l_i, u_i) \gets$ variable bounds of variable $i$
        \If{$l_i = -\infty \;\land\; u_i = +\infty$}
            \State $\hat{x}_i \gets 0$
        \ElsIf{$l_i > -\infty \;\land\; u_i = +\infty$}
            \State $\hat{x}_i \gets l_i$
        \ElsIf{$l_i = -\infty \;\land\; u_i < +\infty$}
            \State $\hat{x}_i \gets u_i$
        \Else
            \If{$c_i \leq -\epsilon$}
                \State $\hat{x}_i \gets l_i$
            \ElsIf{$c_i \geq \epsilon$}
                \State $\hat{x}_i \gets u_i$
            \Else
                \State $\hat{x}_i \gets$ \Call{RandomInteger}{$l_i, u_i$}
            \EndIf
        \EndIf
    \EndFor
    \State \Return $[\hat{x},\hat\Delta^+,\hat\Delta^-]$
\EndFunction
\end{algorithmic}
\end{algorithm}
The initialization procedure in Algorithm \ref{alg:max_feasSol} operates as follows:
\begin{itemize}
    \item If a variable has no finite bounds, it is set to $0$.
    \item If a variable has only one finite bound, it is set to that bound.
    \item If both bounds are finite:
    \begin{itemize}
        \item set the variable to the lower bound if $c_i \leq -\epsilon$, i.e. the coefficient is negative,
        \item set it to the upper bound if $c_i \geq \epsilon$, i.e. the coefficient is positive,
        \item choose a random integer within $[l_i, u_i]$ if $-\epsilon \leq c_i \leq \epsilon$, i.e.the coefficient is close to zero.
    \end{itemize}
\end{itemize}
In this way, the solution initialization is almost entirely deterministic and computationally lightweight, although feasibility with respect to the original MIP problem may be partially compromised. Furthermore, the proposed strategy is parameter-free, which makes it particularly well-suited for the purposes of this thesis.

\section{Additional PACS Optimizations}
While the previous modifications focused on tuning or slightly adjusting specific components of the PACS workflow, the following procedures—analyzed in Section \ref{sec:test_slack_UB}, \ref{sec:test_bdg_constr}, \ref{sec:test_walkMIP}—aim to further enhance the quality of the final solution.

\subsection{Slack Upper Bound Enforcement and Budget Constraint Removal}\label{sec:slack_UB_BDG_constr}
The first idea is to exploit the values of the slack variables $\Delta^+$ and $\Delta^-$ obtained in a previous iteration to enforce these values as upper bounds.  
In the PACS algorithm, this mechanism is already embedded in the budget constraint $\sum_{i=0}^m \Delta_i^{+}+\Delta_i^{-} \le \sum_{i=0}^m \hat\Delta_i^{+}+\hat\Delta_i^{-}$, which provides only a global upper bound. The proposed strategy, instead, aims at fixing the upper bounds of individual slack variables, thereby assisting subsequent FMIP optimizations and improving feasibility.  
The procedure is implemented as shown in Algorithm \ref{alg:fixing_slackUB}.  
\begin{algorithm}[H]
\caption{Fixing Slack Variables to Upper Bound}\label{alg:fixing_slackUB}
\begin{algorithmic}[1]
\Require Original $MIP$ formulation; Solution $[\hat{x}, \hat{\Delta}^+, \hat{\Delta}^-]$; Zero-tolerance $\epsilon$
\Ensure Set $S_{UB}$ of variable–upper bound pairs to be fixed
\Function{SlackUpperBoundFixings}{$MIP$, $[\hat{x}, \hat{\Delta}^+, \hat{\Delta}^-]$}
    \State $S_{UB}^+ \gets \emptyset$ \Comment Set of variable-upper bound pairs for $\Delta^+$
    \State $S_{UB}^- \gets \emptyset$ \Comment Set of variable-upper bound pairs for $\Delta^-$
    \For{$i = 1 \to m$}
        \State $u^+_i \gets$ upper bound of $\Delta^+_i$
        \If{$u^+_i - \hat{\Delta}^+_i > \epsilon$}
            \State $S_{UB}^+ \gets S_{UB}^+ \cup \{(i, \Delta^+_i)\}$
        \EndIf
        \State $u^-_i \gets$ upper bound of $\Delta^-_i$
        \If{$u^-_i - \hat{\Delta}^-_i > \epsilon$}
            \State $S_{UB}^- \gets S_{UB}^- \cup \{(i, \Delta^-_i)\}$
        \EndIf
    \EndFor
    \State \Return $[S_{UB}^+, S_{UB}^-]$
\EndFunction
\end{algorithmic}
\end{algorithm}
Here, the solution vector $\hat{x}$ corresponds to the current PACS incumbent---valid in both the parallel and recombination phases---and each slack variable is compared with its respective upper bound. Whenever the value of a slack variable is strictly smaller than its upper bound by more than the tolerance $\epsilon$, the variable index together with its solution value is recorded in either $S_{UB}^+$ or $S_{UB}^-$, depending on the sign of the slack, as a pair $(i, \hat{\Delta}_i)$.
The resulting sets $S_{UB}^+$ and $S_{UB}^-$ are subsequently employed to update the upper bounds of the slack variables, thereby modifying the auxiliary FMIP and OMIP formulations as follows.
\subsubsection{FMIP Reformulation}
\begin{equation}
\begin{cases}
\text{min} \quad & \sum_{i=0}^m \Delta_i^{+}+\Delta_i^{-} \\ 
\text{s.t.} \quad & Ax + I_m\Delta^+ - I_m\Delta^- = b\\ 
& x_i = \hat{x_i}, \; \forall i \in F\\ 
& \Delta^+_i \le \hat{\Delta}_i, \; \forall (i,\hat{\Delta}_i) \in S_{UB}^+\\
& \Delta^-_i \le \hat{\Delta}_i, \; \forall (i,\hat{\Delta}_i) \in S_{UB}^-\\  
& l \le x \le u\\ 
& x_i \in \mathbb{Z}, \; \forall i \in \mathcal{I} \\ 
& \Delta^+ \ge 0, \; \Delta^- \ge 0
\end{cases}
\end{equation}

\subsubsection{OMIP Reformulation}
\begin{equation}
\begin{cases}
\text{min} \quad & c^T x \\ 
\text{s.t.} \quad & Ax + I_m\Delta^+ - I_m\Delta^- = b\\
& \cancel{\sum_{i=0}^m \Delta_i^{+}+\Delta_i^{-} \le \sum_{i=0}^m \hat\Delta_i^{+}+\hat\Delta_i^{-}}\\ 
& x_i = \hat{x}_i, \; \forall i \in F\\ 
& \Delta^+_i \le \hat{\Delta}_i, \; \forall (i,\hat{\Delta}_i) \in S_{UB}^+\\
& \Delta^-_i \le \hat{\Delta}_i, \; \forall (i,\hat{\Delta}_i) \in S_{UB}^-\\   
& l \le x \le u\\ 
& x_i \in \mathbb{Z}, \; \forall i \in \mathcal{I} \\ 
& \Delta^+ \ge 0, \; \Delta^- \ge 0 
\end{cases}
\end{equation}
It is worth noting that the budget constraint in OMIP can now be removed, since the slack upper bound fixing provides a stronger restriction, as will be further confirmed by the results in Section \ref{sec:test_slack_UB}.  

\subsection{WalkMIP: Violated-Constraints-Based Variable Fixing}

It is worthwhile to investigate a more sophisticated and potentially efficient variable fixing strategy, aiming to enrich diversification and consequently increase the likelihood of finding high-quality solutions within a relatively short time span. The WalkMIP strategy adapts ideas from the WalkSAT$^\text{\cite{WalkSAT}}$ algorithm, targeting specifically the variables that cause infeasibility in a partially feasible solution. 

Algorithm \ref{alg:walkMIP} presents the WalkMIP procedure adapted for MIP constraint satisfaction problems.

\begin{algorithm}[htbp]
\caption{Walk-based Repair Heuristic for MIP}\label{alg:walkMIP}
\begin{algorithmic}[1]
\Require Auxiliary formulation $\overline{MIP}$ (FMIP or OMIP); Initial solution $\hat{x}$; Fixing parameter $\rho$; Random walk probability $p \in (0,1)$; Solution kick probability $p_{HUGE\_KICK}$
\Ensure Updated solution $\hat{x}$ and set $F$ of integer indices
\Function{WalkMIP}{$\overline{MIP}$, $\hat{x}$, $\rho$, $p$, $p_{HUGE\_KICK}$}
    \State $violConstr \gets$ Set of indices of violated constraints, under $\hat{x}$
    \State $F \gets \emptyset$
    \If{*First Execution*} $numInitialConstr \gets |violConstr|$\EndIf
    \If{$violConstr = \emptyset$ } \Return \EndIf
    \If{with $\mathbb{P} = p_{HUGE\_KICK}$} 
        \State \Return \Call{RandomFixings}{$\overline{MIP},\rho$}
    \EndIf
    \State $k \gets $\Call{NumConstrToFix}{$numInitialConstr$,$|violConstr|$}
    \Repeat
        \State $c \gets \Call{UniformFrom}{violConstr}$
        \State $[i^*, x^*, \delta^*] \gets$ \Call{MinDamageMove}{$\overline{MIP}, c,\hat{x}$}
        \If{$minDMG \leq -\epsilon$}
            \State $F \gets F \cup i^*$
            \State $\hat{x} \gets \Call{UpdateSol}{\hat{x},i^*,x^*}$\Comment Apply best move to $\hat{x}$
        \ElsIf{with $\mathbb{P} = 1-p \land i^* \neq -1$}
            \State $F \gets F \cup i^*$
            \State $\hat{x} \gets \Call{UpdateSol}{\hat{x},i^*,x^*}$\Comment Apply min-damage move to $\hat{x}$
        \Else 
            \State $i_{rnd} \gets \Call{RandomInteger}{0,n-1}$
            \State $F \gets F \cup i_{rnd}$
            \State $\hat{x} \gets \Call{UpdateSol}{\hat{x}, i_{rnd},\hat{x}_{i_{rnd}}}$ \Comment Randomly fixing a var in $\hat{x}$
        \EndIf
        \State $violConstr \gets$ Set of indices of violated constraints, under $\hat{x}$
        \If{$violConstr = \emptyset$} \textbf{break} \EndIf
    \Until $k$ times

    \If{$k < \rho \cdot n$}
        \State $F \gets  \Call{RandomFixings}{\overline{MIP},\rho- {k\over{n}}}$ \Comment Fix remaining vars (up to $\rho \cdot n$) from $\hat{x}$
    \EndIf
    \State \Return $F$
\EndFunction
\end{algorithmic}
\end{algorithm}

The WalkMIP algorithm is executed independently by each processor, replacing the standard random fixing procedure. The strategy can be decomposed into three main phases:

\begin{enumerate}
    \item \textbf{Initialization and Huge-Kick Phase (lines 1-10 in Algorithm \ref{alg:walkMIP})}: If this is the first execution of the routine, the algorithm records the number of violated constraints under the current solution $x$, which is later used to determine the number of iterations for the main loop. To enhance diversification and escape local minima, a probability $p_{HUGE\_KICK}$ is introduced to randomly fix variables according to $\rho$. In the implementation, this parameter is set to $1/16$.
    
    \item \textbf{Constraint Selection and Optimization Loop (lines 11-29 in Algorithm \ref{alg:walkMIP})}: 
    The number of constraints to target is determined by
    \[
    k(v, v_0) =
    \begin{cases}
    \left\lceil k_{\min} + (k_{\max}-k_{\min}) \cdot \hat{r}^{\,\beta} \right\rceil & \text{if } v > k_{\min}, \\[0.5em]
    v & \text{otherwise},
    \end{cases}
    \]
    where
    \[
    \hat{r} = \min\!\left(1, \max\!\left(0, \tfrac{v}{\max(1, v_0)}\right)\right), 
    \quad k_{\min} = 32,\; k_{\max} = 1024,\; \beta = \tfrac{\sqrt{5}-1}{2}.
    \]
    Here, $v_0$ is the initial number of violated constraints, and $v$ is the current number of violated constraints.
    This rule guarantees that the number of touched constraints never exceeds $k_{\max}$, while adapting to the reduction of violations over time. 
    At each iteration, the loop samples $k$ violated constraints and, for each, selects one variable to modify, in analogy to the WalkSAT strategy. 
    The function \textsc{MinDamageMove} (Algorithm \ref{alg:min_DMG_WalkMIP}) evaluates candidate moves and returns the one that minimizes constraint violation. The update proceeds as follows:
    \begin{itemize}
        \item If a fixing reduces infeasibility, the best move is applied.
        \item Otherwise, with probability $1-p$, the minimal-damage move is applied.
        \item Otherwise, with probability $p$, a random variable is fixed using its current solution value $x_{i_{\mathrm{rnd}}}$.
    \end{itemize}

    
    \item \textbf{Fixing Remaining Variables (lines 30-33 in Algorithm \ref{alg:walkMIP})}: To maintain consistency with the original variable fixing strategy, the total number of fixed variables is set to $\rho \cdot n$. If the previous phases fixed fewer variables, the remaining variables are fixed according to the standard strategy.
\end{enumerate}

\begin{algorithm}[H]
\caption{Minimum Damage Move Selection for a Violated Constraint in WalkMIP}\label{alg:min_DMG_WalkMIP}
\begin{algorithmic}[1]
\Require Auxiliary formulation $\overline{MIP}$ (FMIP or OMIP); Violated constraint $c$; Current solution vector $\hat{x}$
\Ensure $[i^*, x^*, \delta^*]$ with $i^*$ the selected index, $x^*$ its new value, and $\delta^*$ the minimum violation change
\Function{MinDamageMove}{$\overline{MIP}$, $c$, $\hat{x}$}
    \State $[i^*, x^*, \delta^*] \gets [-1,+\infty,+\infty]$
    \For{\textbf{each} variable $i$ in $c$}
        \If{$i$.type = BIN} $\bar{x}_i \gets \neg \hat{x}_i$
        \ElsIf{$i$.type = INT} $\bar{x}_i \gets \hat{x}_i + \Call{RandomInteger}{-1,1}$
        \ElsIf{$i$.type = DOUBLE} $\bar{x}_i \gets \hat{x}_i + \Call{RandomDouble}{-0.5,0.5}$
        \EndIf
        \State $\bar{x}_i \gets \Call{Clamp}{\bar{x}_i,l_i,u_i}$
        \State $\delta \gets \Call{ComputeViolation}{\hat{x},\bar{x}_i}$
        \If{$\delta < \delta^*$}
            \State $[i^*, x^*, \delta^*] \gets [i,\bar{x}_i,\delta]$
        \EndIf
    \EndFor
    \State \Return $[i^*, x^*, \delta^*]$
\EndFunction
\end{algorithmic}
\end{algorithm}
Any modifications of $\rho$ introduced through the dynamic adjustment routine are subsequently propagated to later WalkMIP iterations. Nevertheless, it is important to emphasize that WalkMIP can only be applied when the slack variables are nonzero.Consequently, the original random fixing strategy, described in Section \ref{sec:dyn_var_fix}, remains necessary during OMIP optimization phases where the slack sum is below the zero-tolerance threshold $\epsilon$.
Although the results in Section \ref{sec:test_walkMIP} indicate that this strategy is less effective than the simpler, more straightforward variable fixing approach, it offers valuable insights and suggests a potential direction for future research.