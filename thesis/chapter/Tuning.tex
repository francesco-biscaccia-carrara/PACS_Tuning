% As discussed in the section[\ref{sec:lim_PACS}], the baseline PACS has a some limitations that make it not suitable for the general-purpose environments. Hence, the PACS parameter and PACS itself are tuned up with this aim: make PACS hardware and input independent, in order to be embedded into a state-of-the-art MIP solver-such as IBM ILOG CPLEX or GUROBI-.
% Parameter to tune up: theta, rho, timing 

\section{PACS with Generalized Fixing}
The baseline PACS algorithm enforces both the starting vector construction and the fixing scheme to operate exclusively on the set $\mathcal{I}$ of integer variables. While this restriction may appear efficient and straightforward, it can, in fact, be limiting. In particular, if the MIP instance contains only a small fraction of integer variables relative to the total, focusing solely on them may reduce diversification and cause the algorithm to converge prematurely to a local minimum, which is undesirable in an optimization process.
To overcome this issue, Algorithm \ref{alg:starting_vector} and Algorithm \ref{alg:variable_fixing} are generalized into Algorithm \ref{alg:gen_starting_vector} and Algorithm \ref{alg:gen_variable_fixing}, respectively, thereby allowing both integer and continuous variables to be considered.
\begin{algorithm}[H]
\caption{Generalized Starting vector heuristic}\label{alg:gen_starting_vector}
\begin{algorithmic}[1]
\Require{Percentage of variables to fix $\theta$, $0 < \theta \leq 100$, Fixed bound constant $c_b$}
\Ensure{Starting integer-feasible vector $\hat{x}$}
\State $V :=$ list of \cancel{integer} variables sorted by increasing bound range $u-l$
\State $F := \emptyset$
\While{$\hat{x}$ is not integer feasible \textbf{AND} $F \neq V$}
    \State $\mathcal{K} :=$ top $\theta \%$ of unfixed variables from $V$
    \For{$k \in \mathcal{K}$}
        \State $\hat{x}_k :=$ random integer value between $[\max(l_k, -c_b), \min(u_k, c_b)]$
    \EndFor
    \State $F := F \cup \mathcal{K}$
    \State $[x, \Delta^+, \Delta^-] := \min\{\sum_i \Delta_i^+ + \Delta_i^- \mid A x + I_m \Delta^+ - I_m \Delta^- = b, \; x_j = \hat{x}_j \;\; \forall j \in F\}$
    \State $Q :=$ index set of \cancel{integer} variables of $x$ with integer value
    \State $\hat{x}_q = x_q, \;\; \forall q \in Q$
    \State $F := F \cup Q$
\EndWhile
\State \Return $\hat{x}$
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
\caption{Generalized Variable Fixing Selection Algorithm}\label{alg:gen_variable_fixing}
\begin{algorithmic}[1]
\Require{Fraction of variables to fix $\rho$, $0 < \rho < 1$}
\Ensure{Set of integer indices $F$}
\Function{RandomFixings}{$\rho$}
    \State $i :=$ random element in $\{1\dots n\}$ \Comment $n$: number of variables in the original MIP 
    \State $F :=$ first $\rho \cdot n$ consecutive \cancel{integer} variable indices starting from $i$ in a circular fashion
    \State \Return $F$
\EndFunction
\end{algorithmic}
\end{algorithm}
Since these algorithms are executed on the auxiliary MIP problems -FMIP or OMIP-, the variables subject to fixing correspond exactly to those defined in the original MIP formulation.  
By generalizing the fixing strategy to include both integer and continuous variables, diversification is enhanced, thereby reducing the likelihood of stagnation in local minima and potentially improving the exploration of the solution space.

\section{Architecture-Agnostic Parallelization}
In the original study, the Message Passing Interface (MPI) was employed to synchronize processors at each recombination phase. While this approach is well suited to high-performance computing environments, it may be unnecessarily complex in general-purpose scenarios, where a simpler multi-threading implementation is often preferable.  
In this thesis, communication is instead managed through a set of logical threads, which may differ from the number of available hardware threads. This abstraction ensures that, even on machines with fewer physical cores, the algorithm can reproduce the same behavior across different architectures, provided sufficient computational time is allowed.  
More specifically, during the coordination phase, either the most feasible or the most optimal solution—depending on whether a recombination FMIP or OMIP is performed—is shared among the logical processors. Each processor then continues working independently on its own copy, with updates to the incumbent solution handled exclusively through a thread-safe update function, formally described in Algorithm~\ref{alg:inc_update}.  
\begin{algorithm}[H]
\caption{Parallel ACS Incumbent Update Procedure}\label{alg:inc_update}
\begin{algorithmic}[1]
\Require Candidate solution $x$ with slack sum $S(x)$ and objective value $C(x)$; Incumbent $\tilde{x}$; Zero-tolerance $\epsilon$
\Ensure Updated incumbent $\tilde{x}$ in a thread-safe manner
\Function{UpdateIncumbent}{$x$}
    \State acquire lock
    \If{$(|S(x)| < |S(\tilde{x})|) \;\;\lor\;\; (|S(x)| < \epsilon \;\land\; C(x) < C(\tilde{x}))$}
        \State $\tilde{x} \gets x$
    \EndIf
    \State release lock
\EndFunction
\end{algorithmic}
\end{algorithm}

This mechanism is crucial to ensure that the algorithm consistently improves and converges within the given time limit. The incumbent solution is updated whenever a better solution is identified: either one with a smaller slack sum, indicating improved feasibility, or one with a lower objective value $c^T x$ provided that the slack sum is less than the tolerance parameter $\epsilon$, the zero-feasibility threshold.

\section{Eliminating Calibration in PACS Parameter Selection}
After adapting the PACS algorithm to a more general-purpose environment, another important challenge arises: parameter selection. Since PACS must be applicable to a wide range of hard MIP instances, it is necessary to identify parameter settings that generally perform well, both in terms of computational efficiency and heuristic solution quality.  
The goal is to remove the need for an explicit calibration phase, while still ensuring reliable performance. The tuning process specifically concerns the following parameters:
\begin{enumerate}
    \item The time span assigned to each sub-MIP, either FMIP\_LNS or OMIP\_LNS
    \item The parameter $\rho$ governing the fixing strategy 
    \item The parameter $\theta$ governing the starting vector
\end{enumerate}

\subsection{Adaptive Determination of Sub-MIP Time Span}
To guarantee determinism in the implementation, each sub-MIP is assigned both a time limit equal to the remaining computation time and a deterministic time limit. The latter is defined as the maximum number of instructions that the solver may execute before termination.  
The deterministic time limit is computed according to the following formula:
\begin{equation}
TL_{DET} = \max\Big(x, \min\Big(\frac{nz}{y}, X\Big)\Big)
\end{equation}
which provides a dynamic mechanism for adapting the computational effort of each sub-MIP.  
In this formulation, $x$ and $X$ denote the minimum and maximum allowable deterministic time limits, respectively, $nz$ represents the number of nonzeros in the constraint matrix $A$ of the MIP problem, and $y$ acts as a scaling factor. The chosen parameter values are:
\begin{enumerate}
\item Minimum deterministic time limit: $x = 10^3$  
\item Scaling factor: $y = 10^2$  
\item Maximum deterministic time limit: $X = 10^7$  
\end{enumerate}
This dynamic adjustment removes the need to explicitly set a fixed time limit for each subproblem, thereby eliminating the necessity of parameter tuning in this respect.

\subsection{Adaptive Variable Fixing through $\rho$ Adjustment}\label{sec:dyn_var_fix}
\subsubsection{Fixed $\rho$ Initialization}
As discussed in Section \ref{sec:PACS_var_fix}, selecting an appropriate variable fixing scheme is a non-trivial task. In particular, the random fixings scheme presents additional challenges: if the fraction $\rho$ of variables to be fixed is set too high, the procedure may fail to yield meaningful improvements; conversely, if $\rho$ is set too low, the resulting search space may become excessively large, making it computationally intractable within a reasonable time frame. \\
For this reason, in the first instance, the parameter $\rho$ is selected from a set of predetermined candidate values. Anticipating the results presented in Section \ref{}, it can be observed that certain values of $\rho$ are more effective in terms of solution quality, as they grant the solver greater flexibility during the optimization process.
\subsubsection{Dynamic Adjustment of Parameter $\rho$}
Since the LNS heuristics in the PACS algorthm restrict the search space by fixing a number of variables according to the value of $\rho$, it is natural to design a mechanism for dynamically adapting this parameter in order to increase the likelihood of discovering high-quality solutions.  
\begin{algorithm}[H]
\caption{Parallel ACS Rho Update (Parallel Phases)}\label{alg:rho_update_MT}
\begin{algorithmic}[1]
\Require Status code $MIP_{code}$ returned by the solver; Adjustment step $\Delta_\rho$ for $\rho$; The variable fixing parameter $\rho$; Number of parallel sub-MIPs $num_{MIP}$
\Ensure Updated value of $\rho$, synchronized across parallel sub-MIPs
\Function{DynRhoUpdate}{$MIP_{code}$, $\Delta_\rho$, $\rho$, $num_{MIP}$}
    \State $\hat\Delta_\rho \gets {\Delta_\rho \over{num_{MIP}}}$
    \State acquire lock
    \If{$MIP_{code} = OPT \;\lor\; MIP_{code} = OPT_{TOL}$}
        \State $\rho \gets \rho - \hat\Delta_\rho$
    \EndIf
    \If{$MIP_{code} = FEAS_{TL} \;\lor\; MIP_{code} = FEAS_{DET\_TL}$}
        \State $\rho \gets \rho + \hat\Delta_\rho$
    \EndIf
    \State Clap $\rho$ within $[0.01,0.99]$
    \If{*Tie Case detected*}
        \State $\rho \gets \rho - \Delta_\rho$
    \EndIf
    \State release lock
\EndFunction
\end{algorithmic}
\end{algorithm}
The procedure, described in Algorithm \ref{alg:rho_update_MT}, is applied after each sub-MIP optimization phase in the parallel step. Based on the status returned by the solver, the value of $\rho$ is updated as follows:
\begin{itemize}
    \item  If the solver hits the time limit—either the deterministic limit or the global remaining time—while still producing a feasible solution, $\rho$ is increased by $\frac{\Delta_\rho}{num_{MIP}}$. This adjustment suggests fixing more variables in subsequent phases, thereby simplifying the subproblem to be solved. 
    \item Conversely, if the solver converges to an optimal solution within the tolerance, this indicates that the corresponding region of the search space has already been sufficiently explored. In this case, $\rho$ is decreased by $\frac{\Delta_\rho}{num_{MIP}}$, enlarging the search space and granting the solver greater freedom in the following optimization steps.  
\end{itemize}
Since each sub-MIP independently attempts to modify the value of $\rho$, synchronization through locking is required to prevent inconsistencies. The final update is determined as the average of the adjustments proposed by the parallel optimization phases. In case of a tie, a deterministic rule is applied: $\rho$ is decreased by $\Delta_\rho$.  
\begin{algorithm}[H]
\caption{Parallel ACS Rho Update (Recombination Phases)}\label{alg:rho_update}
\begin{algorithmic}[1]
\Require Status code $MIP_{code}$ returned by the solver; Adjustment step $\Delta_\rho$ for $\rho$; The variable fixing parameter $\rho$; Number of parallel sub-MIPs $num_{MIP}$
\Ensure Updated value of $\rho$ after recombination adjustment
\Function{DynRhoUpdate}{$MIP_{code}$, $\Delta_\rho$, $\rho$, $num_{MIP}$}
    \State $\hat\Delta_\rho \gets {2\Delta_\rho \over{num_{MIP}}}$
    \If{$MIP_{code} = OPT \;\lor\; MIP_{code} = OPT_{TOL}$}
        \State $\rho \gets \rho - \hat\Delta_\rho$
    \EndIf
    \If{$MIP_{code} = FEAS_{TL} \;\lor\; MIP_{code} = FEAS_{DET\_TL}$}
        \State $\rho \gets \rho + \hat\Delta_\rho$
    \EndIf
    \State Clap $\rho$ within $[0.01,0.99]$
\EndFunction
\end{algorithmic}
\end{algorithm}
For the recombination phase, the procedure is slightly modified into Algorithm \ref{alg:rho_update}, where the adjustment step is doubled, i.e. $2\Delta_\rho / num_{MIP}$, in order to resolve potential tie cases.\\
Finally, although an initial value of $\rho$ must be provided to start the fixing process, experimental results in Section \ref{} show that performance is only marginally affected by this initialization. Consequently, the effectiveness of the method does not critically depend on the initial choice of $\rho$.

\subsection{Parameter-Free Strategy for Initial Solution Construction}
Another non-trivial challenge concerns the selection of the initial solution vector. The parameter $\theta$, which regulates the trade-off between solution quality and execution time, must be tuned to suit the most general cases. Similar to the fixing strategy, the analysis considers a set of predetermined values for $\theta$, while also introducing a novel initialization strategy for the starting solution vector.  
The motivation is that using a small value of $\theta$—which corresponds to generating a high-quality initial solution—may waste computation time, since the true strength of PACS lies in its diversification and parallelization capabilities.  

Therefore, Algorithm \ref{alg:max_feasSol} is designed with the following goal: to provide a heuristic starting solution that is deterministic and computationally lightweight, while remaining reasonably feasible for the original MIP problem.  
\begin{algorithm}[H]
\caption{Heuristic Initialization of Starting Solution}\label{alg:max_feasSol}
\begin{algorithmic}[1]
\Require $MIP$ instance with $n$ variables, objective coefficients $c_i$, bounds $[l_i, u_i]$; zero-tolerance $\epsilon$
\Ensure Starting solution vector $x \in \bar{\mathbb{R}}^n$
\Function{StartSolMaxFeas}{$MIP$}
    \State $x \gets ( +\infty, \ldots, +\infty ) \in \bar{\mathbb{R}}^n$
    \For{$i = 1 \to n$}
        \State $(l_i, u_i) \gets$ variable bounds of variable $i$
        \If{$l_i = -\infty \;\land\; u_i = +\infty$}
            \State $x_i \gets 0$
        \ElsIf{$l_i > -\infty \;\land\; u_i = +\infty$}
            \State $x_i \gets l_i$
        \ElsIf{$l_i = -\infty \;\land\; u_i < +\infty$}
            \State $x_i \gets u_i$
        \Else
            \If{$c_i \leq -\epsilon$}
                \State $x_i \gets l_i$
            \ElsIf{$c_i \geq \epsilon$}
                \State $x_i \gets u_i$
            \Else
                \State $x_i \gets$ \Call{RandomInteger}{$l_i, u_i$}
            \EndIf
        \EndIf
    \EndFor
    \State \Return $x$
\EndFunction
\end{algorithmic}
\end{algorithm}
The initialization procedure in Algorithm \ref{alg:max_feasSol} operates as follows:
\begin{itemize}
    \item If a variable has no finite bounds, it is set to $0$.
    \item If a variable has only one finite bound, it is set to that bound.
    \item If both bounds are finite:
    \begin{itemize}
        \item set the variable to the lower bound if $c_i \leq -\epsilon$,i.e. the coefficient is negative,
        \item set it to the upper bound if $c_i \geq \epsilon$, i.e.the coefficient is positive,
        \item choose a random integer within $[l_i, u_i]$ if $-\epsilon \leq c_i \leq \epsilon$, i.e.the coefficient is close to zero.
    \end{itemize}
\end{itemize}
In this way, the solution initialization is almost entirely deterministic and computationally lightweight, although feasibility with respect to the original MIP problem may be partially compromised. Furthermore, the proposed strategy is parameter-free, which makes it particularly well-suited for the purposes of this thesis.

\section{Additional PACS Optimizations}
While the previous modifications focused on tuning or slightly adjusting specific components of the PACS workflow, the following procedures—analyzed in Section \ref{}—aim to further enhance the quality of the final solution.

\subsection{Slack Upper Bound Enforcement and Budget Constraint Removal}
The first idea is to exploit the values of the slack variables $\Delta^+$ and $\Delta^-$ obtained in a previous iteration to enforce these values as upper bounds.  
In the PACS algorithm, this mechanism is already embedded in the budget constraint $\sum_{i=0}^m \Delta_i^{+}+\Delta_i^{-} \le \sum_{i=0}^m \hat\Delta_i^{+}+\hat\Delta_i^{-}$, which provides only a global upper bound. The proposed strategy, instead, aims at fixing the upper bounds of individual slack variables, thereby assisting subsequent FMIP optimizations and improving feasibility.  
The procedure is implemented as shown in Algorithm \ref{alg:fixing_slackUB}.  
\begin{algorithm}[H]
\caption{Fixing Slack Variables to Upper Bound}\label{alg:fixing_slackUB}
\begin{algorithmic}[1]
\Require $MIP$ model with $n$ variables; Solution $x$ of length $n+2m$; Zero-tolerance $\epsilon$
\Ensure Updated model with adjusted upper bounds for slack variables
\Function{SlackUpperBoundFixing}{$MIP$, $x$}
    \State $S_{UB} \gets \emptyset$ \Comment Set of variables-upper bound to be updated
    \For{$i = n \to n+2m$}
        \State $u_i \gets$ upper bound of variable $i$
        \If{$u_i - x_i > \epsilon$}
            \State $S_{UB} \gets  S_{UB} \cup (i,x_i)$
        \EndIf
    \EndFor
    \State \Return $S_{UB}$
\EndFunction
\end{algorithmic}
\end{algorithm}
Here, the solution vector $x$ corresponds to the current PACS incumbent—valid in both the parallel and recombination phases—and each slack variable is compared with its respective upper bound. If the variable value is smaller than the existing upper bound, the variable index together with its solution value is added to $S_{UB}$ as a pair $(i, x_i)$.
The resulting set $S_{UB}$ is subsequently employed to update the upper bounds of the slack variables, thereby modifying the auxiliary FMIP and OMIP formulations as follows.

\subsubsection{FMIP Reformulation}
\begin{equation}
\begin{cases}
\text{min} \quad & \sum_{i=0}^m \Delta_i^{+}+\Delta_i^{-} \\ 
\text{s.t.} \quad & Ax + I_m\Delta^+ - I_m\Delta^- = b\\ 
& x_i = \hat{x_i}, \; \forall i \in F\\ 
& \Delta^+_i \le \hat{u}_i, \; \forall (i,\hat{u}_i) \in S_{UB}^+\\
& \Delta^-_i \le \hat{u}_i, \; \forall (i,\hat{u}_i) \in S_{UB}^-\\  
& l \le x \le u\\ 
& x_i \in \mathbb{Z}, \; \forall i \in \mathcal{I} \\ 
& \Delta^+ \ge 0, \; \Delta^- \ge 0
\end{cases}
\end{equation}

\subsubsection{OMIP Reformulation}
\begin{equation}
\begin{cases}
\text{min} \quad & c^T x \\ 
\text{s.t.} \quad & Ax + I_m\Delta^+ - I_m\Delta^- = b\\
& \cancel{\sum_{i=0}^m \Delta_i^{+}+\Delta_i^{-} \le \sum_{i=0}^m \hat\Delta_i^{+}+\hat\Delta_i^{-}}\\ 
& x_i = \hat{x}_i, \; \forall i \in F\\ 
& \Delta^+_i \le \hat{u}_i, \; \forall (i,\hat{u}_i) \in S_{UB}^+\\
& \Delta^-_i \le \hat{u}_i, \; \forall (i,\hat{u}_i) \in S_{UB}^-\\  
& l \le x \le u\\ 
& x_i \in \mathbb{Z}, \; \forall i \in \mathcal{I} \\ 
& \Delta^+ \ge 0, \; \Delta^- \ge 0 
\end{cases}
\end{equation}
In these formulations, the set $S_{UB}$ is partitioned into $S_{UB}^+$ and $S_{UB}^-$ to remain consistent with the notation used for FMIP and OMIP.  
It is worth noting that the budget constraint in OMIP can now be removed, since the slack upper bound fixing provides a stronger restriction, as will be further confirmed by the results in Section \ref{}.  

\subsection{WalkMIP: Violated Constraints Based Variable Fixing}
It's worth to investigate a more sophisticated and hopefully efficient way to fix the variables values, enriching the diversification and consequently increasing the chances of find high-quality solution in a relative tiny time span. For this purpose, the WalkMIP strategy takes the strategy from the WalkSAT$^\text{\cite{WalkSAT}}$ strategy and implement it to directly target the variables that cause the infeasibility of a partially feasible solution. 
The Algorithm \ref{alg:walkMIP} is the suiting of WalkMIP algorithm to cover constraint satisfability problem.
\begin{algorithm}[H]
\caption{Walk-based Repair Heuristic for MIP}\label{alg:walkMIP}
\begin{algorithmic}[1]
\Require $MIP$ model with $n+2m$ variables (FMIP or OMIP); Initial solution $x$; Fixing parameter $\rho$; Random walk probability $p \in (0,1)$; Solution kick probability $p_{HUGE\_KICK}$
\Ensure Updated model with improved feasible solution
\Function{WalkMIP}{$MIP$, $x$, $\rho$, $p$,$p_{HUGE\_KICK}$}
    \State $violConstr \gets$ Set of indices of violated constraints, under $x$
    \If{*First Execution*} $numInitialConstr \gets |violConstr|$\EndIf
    \If{$violConstr = \emptyset$ } \Return \EndIf
    \If{with probability $p_{HUGE\_KICK}$} { \Call{RandomFixings}{$\rho$}}
    \EndIf
    \State $k \gets $\Call{NumVarToFix}{$numInitialConstr$,$|violConstr|$}
    \Repeat
        \State $c \gets $ Random violated constraint, $c \in violConstr$
        \State $(i_{best}, val_{best}, minDMG) \gets$ \Call{MinDamageMove}{$c, x$}
        \If{$minDMG \leq -\epsilon$}
            \State $x \gets \Call{UpdateSol}{x,i_{best},val_{best}}$\Comment Apply best move to $x$
        \ElsIf{with probability $1-p \land i_{best} \neq -1$}
             \State $x \gets \Call{UpdateSol}{x,i_{best},val_{best}}$\Comment Apply min-damage move to $x$
        \Else 
            \State $x \gets $ Fix a random var in $x$
        \EndIf
        \State $violConstr \gets$ Set of indices of violated constraints, under $x$
        \If{$violConstr = \emptyset$} \textbf{break} \EndIf
    \Until $k$ times

    \If{$k < \rho \cdot n$}
        \State Fix remaining variables (up to $\rho \cdot n_{\text{mip}}$) from original $x$
    \EndIf
\EndFunction

\Function{MinDamageMove}{$c, x$}
    \State $minDMG \gets +\infty$, $i_{best} \gets -1$, $val_{best} \gets +\infty$
    \For{\textbf{each} variable $i$ in $c$}
        \If{$i$.type = BIN} $\hat{x}_i \gets \neg x_i$
        \ElsIf{$i$.type = INT} $\hat{x}_i \gets x_i + \Call{RandomInteger}{-1,1}$
        \ElsIf{$i$.type = DOUBLE} $\hat{x}_i \gets x_i + \Call{RandomDouble}{-0.5,0.5}$
        \EndIf
        \State Clamp $\hat{x}_i$ within its bounds $[l_i,u_i]$
        \State $\delta \gets \Call{ComputeViolation}{x,\hat{x}_i}$
        \If{$\delta < minDMG$}
            \State $i_{best} \gets i$, $minDMG \gets \delta$, $val_{best} \gets \hat{x}_i$
        \EndIf
    \EndFor
    \State \Return $(i_{best}, val_{best}, minDMG)$
\EndFunction
\end{algorithmic}
\end{algorithm}





It's important to notice that this procedure itself could only be used in a context in which the slack variable are different from zero and therefore, the original random fixing strategy, presented in the Section \ref{sec:dyn_var_fix}, must be still utilized in the OMIP optimization phases in which the slack sum is equal to zero.


%UNfortunetely 